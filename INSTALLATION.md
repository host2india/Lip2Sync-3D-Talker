ğŸ“˜ INSTALLATION.md
Lip2Sync-3D-Talker â€“ Installation & Setup Guide

This guide explains how to install, configure, and run Lip2Sync-3D-Talker on:

âœ” Local Linux Laptop / Desktop

âœ” Google Colab GPU

âœ” Cloud Servers (RunPod / DigitalOcean)

ğŸš€ 1. Requirements
System Requirements
Item	Requirement
OS	Ubuntu/Linux recommended
Python	3.9 â€“ 3.10
GPU	Optional â€” CUDA improves speed 20Ã—
RAM	8GB+ recommended
Disk	10GB recommended
ğŸ“¦ 2. Install Dependencies
2.1 Create a Virtual Environment (optional but recommended)
python3 -m venv venv
source venv/bin/activate

2.2 Install Python packages
pip install -r requirements.txt


If you encounter missing packages, install manually:

pip install torch torchvision torchaudio
pip install gfpgan realesrgan facexlib

ğŸ“ 3. Add Required Model Files

Download the following checkpoints manually.

Purpose	File	Required
3D Reconstruction	epoch_20.pth	âœ”
Expression model	audo2exp_00300-model.pth	âœ”
Pose model	audio2pose_00140-model.pth	âœ”
Mapping network	mapping_00109-model.pth.tar	âœ”
Main renderer	epoch_00190.pth.tar	âœ”
Hubert model	hubert_soft.pt	âœ”
GFPGAN	GFV.pth	âœ”
Morphable model	Entire BFM_Fitting folder	âœ”
(Optional) FaceVid2Vid	facevid2vid_00189-model.pth.tar	âš  Optional
(Optional) 68 Landmarks	shape_predictor_68_face_landmarks.dat	âš  Optional
Place all files here:
models/sadtalker/checkpoints/


Folder example:

models/
 â””â”€â”€ sadtalker/
     â””â”€â”€ checkpoints/
         â”œâ”€â”€ epoch_20.pth
         â”œâ”€â”€ epoch_00190.pth.tar
         â”œâ”€â”€ audo2exp_00300-model.pth
         â”œâ”€â”€ audio2pose_00140-model.pth
         â”œâ”€â”€ mapping_00109-model.pth.tar
         â”œâ”€â”€ mapping_00229-model.pth.tar
         â”œâ”€â”€ hubert_soft.pt
         â”œâ”€â”€ GFV.pth
         â”œâ”€â”€ BFM_Fitting/
         â””â”€â”€ shape_predictor_68_face_landmarks.dat

ğŸ¬ 4. Run the Engine Locally
Test script:
from app.engines.sadtalker.engine import SadTalkerEngine

engine = SadTalkerEngine()

out = engine.infer_from_files(
    "image.png",
    "audio.wav"
)

print("Generated Output:", out)


Videos will appear in:

app/engines/sadtalker/output/

âš¡ 5. Run the API Server

Install FastAPI & uvicorn:

pip install fastapi uvicorn


Start server:

uvicorn api:app --host 0.0.0.0 --port 3000

ğŸ¤– 6. Run on Google Colab (GPU)

Create a new notebook and paste:

!git clone https://github.com/host2india/Lip2Sync-3D-Talker.git
%cd Lip2Sync-3D-Talker

!pip install -r requirements.txt

# Download checkpoints manually or mount drive


Then run inference using:

from app.engines.sadtalker.engine import SadTalkerEngine
e = SadTalkerEngine(device="cuda")
e.infer_from_files("image.png", "audio.wav")

ğŸ›  7. Troubleshooting
CUDA not used

Ensure device=â€œcudaâ€ in config.yaml

Check GPU:

import torch
print(torch.cuda.is_available())

Missing checkpoints

Engine will warn:

FileNotFoundError: mapping_00109-model.pth.tar


â†’ means model file missing.

Slow inference

CPU mode is 10Ã—â€“25Ã— slower

Use Google Colab GPU for speed

ğŸ¯ 8. Recommended Usage

Run inference on Colab GPU

Use this repo to package your engine

Integrate into a backend through FastAPI
